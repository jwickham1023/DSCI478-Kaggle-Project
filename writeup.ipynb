{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# DSCI478 Kaggle Project - Credit Card Fraud Detection\n",
    "### Nick Brady, Jakob Wickham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "Note for us:\n",
    "- If you want the PDF to not display a cell, click the three dots on the cell, click \"Add Cell Tag\", and put \"remove_cell\"\n",
    "- If you want to hide the code instead, put \"remove_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, average_precision_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2024 alone, 134 million Americans have been victims of credit card fraud, with unauthorized purchases accounting for 6.2 billion dollars annually. This trend of fraud is on the rise in financial and business challenges. (Reference 1).  While consumers are generally protected from fraudulent transactions, the impact is beyond the negative experience; financial institutions and businesses bear the cost.\n",
    "\n",
    "To reduce these risks, financial institutions leverage Machine Learning (ML) and other statistical models to detect and prevent transactions from occurring. Creating effective fraud detection models has challenges, including correctly classifying fraudulent transactions, minimizing customer impact with false positives, and dealing with multiple fraud vectors.\n",
    "\n",
    "In this project, we will explore the process of creating a fraud detection model, the data set we used, feature engineering, model evaluation metrics, and the challenges of imbalanced data sets. Since fraudulent activity is rare, accuracy metrics are not applicable when determining model evaluation as they mainly give a score on how well it detects *non-fraud*. In this project we will be using precision, recall, F1 score and the Precision-Recall Curve (PR - AUC) to provide an accurate representation of the model given the challenges in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of the information for credit card purchases, we used a synthetic data set (References 2 & 3), which allows us to explore fraud detection techniques while also maintaining privacy and security concerns. At the end of this section is a list of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This synthetic data can help us in research and model experimentation. It introduces limitations: the data may not reflect real world approaches that fraudsters use, customer transactions may lack variability, and `TX_FRAUD_SCENARIO` can create potential data leakage.\n",
    "With those limitations in mind, we want to be able to focus on building feature engineering and methods that can generalize well to actual scenarios. We must first explore the data cleaning and processing steps taken to prepare the data set for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "    \\begin{tabular}{| l | p{10cm} |}\n",
    "        \\hline\n",
    "        \\textbf{Feature} & \\textbf{Description} \\\\\n",
    "        \\hline\n",
    "        TRANSACTION\\_ID & A unique identifier for the transaction \\\\\n",
    "        \\hline\n",
    "        TX\\_DATETIME & Date and time at which the transaction occurs \\\\\n",
    "        \\hline\n",
    "        CUSTOMER\\_ID & The identifier for the customer. Each customer has a unique identifier \\\\\n",
    "        \\hline\n",
    "        TERMINAL\\_ID & The identifier for the merchant (or, more precisely, the terminal). Each terminal has a unique identifier \\\\\n",
    "        \\hline\n",
    "        TX\\_AMOUNT & The amount of the transaction \\\\\n",
    "        \\hline\n",
    "        TX\\_FRAUD & A binary variable with the value for a legitimate transaction or the value for a fraudulent transaction \\\\\n",
    "        \\hline\n",
    "        TX\\_TIME\\_SECONDS & Timestamp of transaction in seconds \\\\\n",
    "        \\hline\n",
    "        TX\\_TIME\\_DAYS & Timestamp of transaction in days \\\\\n",
    "        \\hline\n",
    "        TX\\_FRAUD\\_SCENARIO & Numerical indicator of the type of fraud scenario \\\\\n",
    "        \\hline\n",
    "    \\end{tabular}\n",
    "    \\caption{Transaction Features and Their Descriptions}\n",
    "    \\label{tab:transaction_features}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting exploratory data analysis and model development, we had to clean data and process the data.\n",
    "\n",
    "Since this data was synthetically generated, no missing (NaN) values were present. In the data, the `TX_FRAUD_SCENARIO` column was dropped to prevent data leakage as it contained information directly to the fraud classification. `TX_DATETIME` was transformed into multiple features--`TX_HOUR` (hour of the transaction) and `TX_DAYOFWEEK` (day of the transaction week)--to capture temporal patterns. Once that transformation was completed, `TX_DATETIME` was dropped due to redundant data.\n",
    "\n",
    "Completing these preprocessing steps ensure that the dataset was structured for feature extraction while also minimizing data leakage. Transforming `TX_DATETIME` allowed us to capture temporal fraud patterns. With these preprocessing steps done, the data set is now ready for exploratory data analysis (EDA), where we will be determining patterns in fraudulent and non-fraudulent transactions, key trends and assess potential predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "\n",
    "df.columns = df.columns.str.strip().str.upper()\n",
    "\n",
    "# Drop unnecessary column\n",
    "df.drop(columns=['TX_FRAUD_SCENARIO'], inplace=True)\n",
    "\n",
    "# Convert TX_DATETIME and extract key time features\n",
    "df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])\n",
    "df['TX_HOUR'] = df['TX_DATETIME'].dt.hour\n",
    "df['TX_DAYOFWEEK'] = df['TX_DATETIME'].dt.dayofweek\n",
    "df.drop(columns=['TX_DATETIME'], inplace=True)\n",
    "\n",
    "\n",
    "# Time-Based Split\n",
    "split_point = df['TX_TIME_DAYS'].quantile(0.8)\n",
    "train_df = df[df['TX_TIME_DAYS'] < split_point].copy()\n",
    "test_df = df[df['TX_TIME_DAYS'] >= split_point].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we performed basic data exploration to learn more about our data set. One of the first things in our exploration analysis that was noticeable was that the data set was highly imbalanced, with only fraudulent activity making up a small proportion of the total (which was a given considering we're working with fraud). Being aware of class balance is important as it affects model performance and evaluation metrics. Additionally, we analyzed the average transaction that customers typically make with their cards. With this information we wanted to determine if going above or below the average transaction was more likely fraudulent. Since this was a significant factor in fraud detection, we further explored whether a customer’s personal average carried more of a determining factor than global average (See Figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the average non-fraud transaction amount for each customer\n",
    "customer_avg_amount = df.groupby(['CUSTOMER_ID'])['TX_AMOUNT'].mean().reset_index()\n",
    "\n",
    "# Calculate the average fraud transaction amount for each customer\n",
    "customer_avg_fraud_amount = df[df['TX_FRAUD'] == 1].groupby(['CUSTOMER_ID'])['TX_AMOUNT'].mean().reset_index()\n",
    "\n",
    "customer_avg_merged = pd.merge(customer_avg_amount, customer_avg_fraud_amount, how='inner', suffixes=('_nonfraud', '_fraud'), on='CUSTOMER_ID').fillna(0)\n",
    "\n",
    "customer_avg_merged[\"ABOVE_AVG\"] = customer_avg_merged[\"TX_AMOUNT_fraud\"] > customer_avg_merged[\"TX_AMOUNT_nonfraud\"]\n",
    "customer_avg_merged[\"BELOW_AVG\"] = ~customer_avg_merged[\"ABOVE_AVG\"]\n",
    "\n",
    "count_above = len(customer_avg_merged[customer_avg_merged[\"ABOVE_AVG\"]])\n",
    "count_below = len(customer_avg_merged[customer_avg_merged[\"BELOW_AVG\"]])\n",
    "percentage_above = len(customer_avg_merged[customer_avg_merged[\"ABOVE_AVG\"]]) / len(customer_avg_merged)\n",
    "percentage_below = len(customer_avg_merged[customer_avg_merged[\"BELOW_AVG\"]]) / len(customer_avg_merged)\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    'Metric': ['Fraud Above Personal Average', 'Fraud Not Above Personal Average'],\n",
    "    'Count': [count_above, count_below],\n",
    "    'Percentage': [f\"{percentage_above:.2f}%\", f\"{percentage_below:.2f}%\"]\n",
    "})\n",
    "\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our findings showed that incorporating a customer’s historical spending behavior improved the chance of finding fraudulent activity, which could show that people who commit fraud are trying to stay within the expected spending patterns of the individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also investigated whether certain terminals were consistent with fraudulent activity (See Figure 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fraud_df = df[df['TX_FRAUD'] == 1]\n",
    "\n",
    "# Extract unique fraud in the data\n",
    "fraud_dates_sorted = fraud_df[[\"TERMINAL_ID\", \"TX_TIME_DAYS\"]].drop_duplicates().sort_values([\"TERMINAL_ID\", \"TX_TIME_DAYS\"])\n",
    "\n",
    "# Calculate day differences between consecutive fraud events per terminal\n",
    "fraud_dates_sorted[\"PREV_DAY\"] = fraud_dates_sorted.groupby(\"TERMINAL_ID\")[\"TX_TIME_DAYS\"].shift(1)\n",
    "fraud_dates_sorted[\"DAY_DIFF\"] = fraud_dates_sorted[\"TX_TIME_DAYS\"] - fraud_dates_sorted[\"PREV_DAY\"]\n",
    "\n",
    "diffs = fraud_dates_sorted.dropna(subset=[\"DAY_DIFF\"])\n",
    "\n",
    "# Create a table to display the information\n",
    "terminal_gap_summary = diffs.groupby(\"TERMINAL_ID\").agg(\n",
    "    total_gaps=(\"DAY_DIFF\", \"count\"),\n",
    "    consecutive_gaps=(\"DAY_DIFF\", lambda x: (x == 1).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Compute additional statistics\n",
    "terminal_gap_summary[\"gap_ratio\"] = terminal_gap_summary[\"consecutive_gaps\"] / terminal_gap_summary[\"total_gaps\"]\n",
    "terminal_gap_summary[\"non_consecutive_gaps\"] = terminal_gap_summary[\"total_gaps\"] - terminal_gap_summary[\"consecutive_gaps\"]\n",
    "\n",
    "# Classify fraud patterns\n",
    "terminal_gap_summary[\"fraud_pattern\"] = terminal_gap_summary[\"gap_ratio\"].apply(\n",
    "    lambda r: \"Consecutive\" if r >= 0.5 else \"Non-Consecutive\"\n",
    ")\n",
    "\n",
    "# Count fraud\n",
    "fraud_counts = fraud_df.groupby(\"TERMINAL_ID\").size().reset_index(name=\"fraud_count\")\n",
    "\n",
    "# Merge tables\n",
    "merged_summary = pd.merge(terminal_gap_summary, fraud_counts, on=\"TERMINAL_ID\", how=\"left\")\n",
    "\n",
    "# Compute summary statistics by fraud pattern\n",
    "pattern_summary = merged_summary.groupby(\"fraud_pattern\").agg(\n",
    "    Terminal_Count=(\"TERMINAL_ID\", \"count\"),\n",
    "    Fraud_Count=(\"fraud_count\", \"sum\"),\n",
    ").reset_index()\n",
    "\n",
    "# Compute additional statistics\n",
    "pattern_summary[\"Terminal_Percentage\"] = pattern_summary[\"Terminal_Count\"] / pattern_summary[\"Terminal_Count\"].sum() * 100\n",
    "pattern_summary[\"Fraud_Percentage\"] = pattern_summary[\"Fraud_Count\"] / pattern_summary[\"Fraud_Count\"].sum() * 100\n",
    "\n",
    "pattern_summary = pattern_summary.round(2)\n",
    "\n",
    "print(pattern_summary.set_index(\"fraud_pattern\").transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This revealed that a small number of terminals were responsible for a large portion of fraud, often occurring over consecutive days. However, there was no fixed pattern in the number of days fraud occurred and no clear distinction between whether a certain day should be a concern (See Figure 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(8,4))\n",
    "\n",
    "consecutive_counts = diffs[diffs[\"DAY_DIFF\"] == 1][\"TERMINAL_ID\"].value_counts()\n",
    "non_consecutive_counts = diffs[diffs[\"DAY_DIFF\"] > 1][\"TERMINAL_ID\"].value_counts()\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.histplot(x=consecutive_counts, bins=30, kde=False, ax=ax, color=\"brown\")\n",
    "sns.histplot(x=non_consecutive_counts, bins=20, kde=False, ax=ax, color='skyblue')\n",
    "ax.set_xlim([0, max(consecutive_counts.max(), non_consecutive_counts.max())])\n",
    "ax.set(xlabel = \"Number of fraudulent transactions\", ylabel=\"Frequency\",\n",
    "       title=\"Distribution of Consecutive and Non-Consecutive Days Per Terminal\")\n",
    "ax.legend([\"Consecutive Days\", \"Non-Consecutive Days\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, we had to implement a way in our classification approach that acknowledges that past fraudulent activity at a terminal is a strong factor, but not all future transactions at that terminal should be flagged as fraud.\n",
    "\n",
    "Our exploration analysis showed many key significant insights into fraud detection, which include class imbalance, transaction amount and terminal level indicators. Those findings gave us information that was necessary to implement our feature engineering and modeling approach to account for the customer and terminal behavior patterns and minimize our bias in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a robust fraud detection, we engineered features that capture both population fraud trends and individual customer-based spending behaviors. This creates a model that can detect a wide range of fraud tactics while also identifying personalized anomalies that could indicate fraudulent activity at a customer level.\n",
    "\n",
    "Transaction amount serves as a critical indication in fraudulent activity, so we'd want to try and capture this. We applied a Z-score normalization to standardize transactions relative to the dataset mean, which ensures that large transaction values were flagged as unusual. However, some of the fraudulent activities attempted to fall within the customer expected ranges but still would appear abnormal compared to overall transaction patterns. Since specific terminals were exploited, we implemented rolling seven-and twenty-eight day rolling fraud rates. Acknowledging that a terminal who has had fraudulent activity doesn’t mean they will always be fraudulent we implemented an exponential decay factor to ensure old occurrences of fraud had a diminishing influence in our model and attempting to prevent model but still capturing consistent fraud risk. To further improve our population fraud trend, we implemented a Local Outlier Factor (LOF), which is an unsupervised anomaly detection method that assigned a likelihood of fraud score based on its behavior. This method creates context-based detecting and maintains to not solely rely on extreme transaction values. During the feature engineering process we considered using an Isolation Forest for our unsupervised choice, but the anomaly score made our model rely on the Isolation Forest prediction, which reduced its ability to learn other fraudulent trends.\n",
    "\n",
    "Listed below shows all population-based features that were created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "    \\begin{tabular}{| l | p{10cm} |}\n",
    "        \\hline\n",
    "        \\textbf{Feature} & \\textbf{Purpose} \\\\\n",
    "        \\hline\n",
    "        TX\\_AMOUNT\\_Z\\_Score & Check for extreme transaction amounts compared to data set's mean \\\\\n",
    "        \\hline\n",
    "        TX\\_AMOUNT\\_Percentile & Check transaction amounts relative to all others \\\\\n",
    "        \\hline\n",
    "        TERMINAL\\_FRAUD\\_RATIO & Terminal-level fraud check that decays over time \\\\\n",
    "        \\hline\n",
    "        ANOMALY\\_SCORE (LOF) & Unsupervised learning to apply a score beyond deviation of previous features \\\\\n",
    "        \\hline\n",
    "    \\end{tabular}\n",
    "    \\caption{Feature Description and Purpose}\n",
    "    \\label{tab:feature_purpose}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population-based trend provides a big picture view of our sample, but fraudulent transactions often are different from customer to customer rather than the population itself. To recognize this, we engineered features that will adapt to each customer's unique spending habits instead of over relying on dataset wide fraud indicators. The key feature of detecting fraud at a customer level is track spending behavior over time. We computed a 14-day rolling window that tracks whether a customer suddenly makes a significant change in their spending habits. This can help with identifying fraud scenarios where fraudsters make large transactions over a short period, deviation from previous spending habits. We normalized the transaction amount based on each customer's historical data instead of the entire data set. This implementation prevents the model from flagging frequently high spending larger transactions, while still detecting unexpected large purchases from lower spending customers.\n",
    "\n",
    "The table below shows all customer-based features that were created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "    \\begin{tabular}{| l | p{10cm} |}\n",
    "        \\hline\n",
    "        \\textbf{Customer-Based Features} & \\textbf{Purpose} \\\\\n",
    "        \\hline\n",
    "        SPENDING\\_RATIO\\_CHANGE & Checks for sudden shifts in customer spending behavior \\\\\n",
    "        \\hline\n",
    "        SPENDING\\_Z\\_SCORE\\_28D & Identifies how unusual a transaction is for a specific customer based on their historical spending patterns \\\\\n",
    "        \\hline\n",
    "    \\end{tabular}\n",
    "    \\caption{Customer-Based Features and Their Purpose}\n",
    "    \\label{tab:customer_features}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at both the population and customer-based fraud detection the purpose is to create a model that can identify fraud trends that are occurring at large scale, but also adapting to customer-level behavior. This combination with dynamic adjusting and personalized profiles attempts to create a fraud detection model that is both effective and adaptable at identifying ever changing fraud tactics.\n",
    "With the creation of these engineered features, we now will focus on model selection and evaluating different approaches at models to classify fraudulent activity effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Feature Engineering Rolling Windows with decay\n",
    "scaler = StandardScaler()\n",
    "train_df['TX_AMOUNT_Z_SCORE'] = scaler.fit_transform(train_df[['TX_AMOUNT']])\n",
    "test_df['TX_AMOUNT_Z_SCORE'] = scaler.transform(test_df[['TX_AMOUNT']])\n",
    "\n",
    "train_df['TX_AMOUNT_PERCENTILE'] = train_df['TX_AMOUNT'].rank(pct=True)\n",
    "test_df['TX_AMOUNT_PERCENTILE'] = test_df['TX_AMOUNT'].rank(pct=True)\n",
    "\n",
    "# Optimize Decay Factor \n",
    "decay_values = np.linspace(0.8, 0.99, 20)  \n",
    "best_decay, best_corr = 0.95, float('-inf')  \n",
    "\n",
    "for decay in decay_values:\n",
    "    temp_df = train_df.copy()\n",
    "    temp_df['TERMINAL_FRAUD_RATIO_28D'] = (\n",
    "        temp_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "        .shift(1)\n",
    "        .ewm(alpha=1-decay)\n",
    "        .mean()\n",
    "    )\n",
    "    correlation = temp_df['TERMINAL_FRAUD_RATIO_28D'].corr(temp_df['TX_FRAUD'])\n",
    "    \n",
    "    if correlation > best_corr:\n",
    "        best_corr = correlation\n",
    "        best_decay = decay\n",
    "\n",
    "optimal_decay = best_decay  # Best decay factor based on correlation\n",
    "\n",
    "# Apply the optimized decay factor\n",
    "train_df['TERMINAL_FRAUD_RATIO_28D'] = (\n",
    "    train_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .ewm(alpha=1-optimal_decay)\n",
    "    .mean()\n",
    ")\n",
    "test_df['TERMINAL_FRAUD_RATIO_28D'] = (\n",
    "    test_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .ewm(alpha=1-optimal_decay)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Terminal Fraud Ratios\n",
    "train_df['TERMINAL_FRAUD_RATIO_7D'] = (\n",
    "    train_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .rolling(7, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "test_df['TERMINAL_FRAUD_RATIO_7D'] = (\n",
    "    test_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .rolling(7, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Customer Spending Trends\n",
    "train_df['SPENDING_RATIO_CHANGE'] = (\n",
    "    train_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: x.pct_change().rolling(14, min_periods=1).mean())\n",
    ")\n",
    "test_df['SPENDING_RATIO_CHANGE'] = (\n",
    "    test_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: x.pct_change().rolling(14, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "train_df['SPENDING_Z_SCORE_28D'] = (\n",
    "    train_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: (x - x.mean()) / (x.std() + 1))\n",
    ")\n",
    "test_df['SPENDING_Z_SCORE_28D'] = (\n",
    "    test_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: (x - x.mean()) / (x.std() + 1))\n",
    ")\n",
    "\n",
    "\n",
    "# Anomaly Detection (Local Outlier Factor)\n",
    "features_for_lof = ['TX_AMOUNT_Z_SCORE', 'SPENDING_RATIO_CHANGE', 'TERMINAL_FRAUD_RATIO_28D']\n",
    "train_df[features_for_lof] = train_df[features_for_lof].fillna(train_df[features_for_lof].median())\n",
    "test_df[features_for_lof] = test_df[features_for_lof].fillna(test_df[features_for_lof].median())\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.005, novelty=True)\n",
    "lof.fit(train_df[features_for_lof].values)\n",
    "\n",
    "train_df['ANOMALY_SCORE'] = lof.decision_function(train_df[features_for_lof].values)\n",
    "test_df['ANOMALY_SCORE'] = lof.decision_function(test_df[features_for_lof].values)\n",
    "\n",
    "# Normalize Anomaly Scores\n",
    "scaler = MinMaxScaler()\n",
    "train_df['ANOMALY_SCORE'] = scaler.fit_transform(train_df[['ANOMALY_SCORE']])\n",
    "test_df['ANOMALY_SCORE'] = scaler.transform(test_df[['ANOMALY_SCORE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "engineered_features = ['TX_AMOUNT_Z_SCORE',\n",
    "                       'TX_AMOUNT_PERCENTILE',\n",
    "                       'TERMINAL_FRAUD_RATIO_7D',\n",
    "                       'TERMINAL_FRAUD_RATIO_28D',\n",
    "                       'SPENDING_RATIO_CHANGE',\n",
    "                       'SPENDING_Z_SCORE_28D',\n",
    "                       'ANOMALY_SCORE']\n",
    "\n",
    "for feature in engineered_features:\n",
    "    median_value = train_df[feature].median()\n",
    "    train_df[feature] = train_df[feature].fillna(median_value)\n",
    "    test_df[feature] = test_df[feature].fillna(median_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the data imbalance is a fundamental challenge in the data set; classifiers would predict the majority class, which is not the main factor that we are trying to classify. To address this, we selected a combination of models that will vary based on interpretability and accuracy: **Balanced Random Forest (BRF)**, **XGBoost**, and a **stacked model**. We also selected a **Logistic Regression** model to see how a simple model would perform too.\n",
    "\n",
    "With each of these models, we used a Local Outlier Factor (LOF), which is an unsupervised anomaly detection model that was implemented in our feature engineering. LOFs generate an anomaly score based on its nearest neighbors in terms of the objective, and give a score based on how significantly different the transactions were compared to the others which assisted the models in classifying fraud.\n",
    "\n",
    "We chose to use BRFs because it would be the most interpretable model to understand how the fraud is detected. It handles imbalanced data well due to the undersampling of the majority class at each tree, but can miss complex fraud patterns with non-linear relationships.\n",
    "XGBoost was chosen because it can handle complex non linear relationships and with scale_pos_weight it can handle imbalances through class weighting, but it relies on gradient boosting which makes individual decisions less clear compared to BRF.\n",
    "Our last model was stacking, which combines both previous models together; It combines the strength and weakness of both models together into a meta-model to combine both of those previous inputs together. Due to the meta-model, the final decisions are not directly interpretable.\n",
    "\n",
    "With the choice of our model made, designed to handle the class imbalance, we need to run each model and evaluate each to choose which model would be best for our objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a little bit about how we're going to be scoring our models. We'll be using 4 scores to determine how well our models perform: Precision, Recall, F1, and a PR-AUC score.\n",
    "\n",
    "- Precision (PPV): Correctly identified fraudulent cases across all *classified* fraudulent cases\n",
    "    - Also called the \"positive predictive value\"\n",
    "- Recall (TPR): Correctly identified fraudulent cases across all *truly* fraudulent cases\n",
    "    - Also called the \"true positive rate\", power, or sensitivity\n",
    "- F1: A metric providing a balanced measure of the harmonic mean of precision and recall\n",
    "- PR-AUC: The model's ability to distinguish between classes\n",
    "\n",
    "The score we'll be mainly looking at is the F1 score as it'll tell us how good our model is at classifying both non-fraud and fraud cases. The higher the score, the better the model is at both *predicting* and *detecting* a fraudulent case.\n",
    "\n",
    "Accuracy scores are not a good evaluation metric with imbalanced data, since it is based on how accurate the model predicts the *majority* class (legitimate transactions), which is not the objective of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def model_run(model: BalancedRandomForestClassifier | XGBClassifier | StackingClassifier,\n",
    "              features = engineered_features, train_df = train_df, test_df = test_df) -> dict[str, float]:\n",
    "    # Train the model\n",
    "    model.fit(train_df[features], train_df['TX_FRAUD'])\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(test_df[features])\n",
    "    y_scores = model.predict_proba(test_df[features])[:,1]\n",
    "\n",
    "    precision = precision_score(test_df['TX_FRAUD'], y_pred)\n",
    "    recall = recall_score(test_df['TX_FRAUD'], y_pred)\n",
    "    f1 = f1_score(test_df['TX_FRAUD'], y_pred)\n",
    "    pr_auc = average_precision_score(test_df['TX_FRAUD'], y_scores)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(test_df['TX_FRAUD'], y_pred).ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'FPR': fpr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "\n",
    "logreg_results = model_run(logreg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "brf_model = BalancedRandomForestClassifier(n_estimators=500, max_depth=15, random_state=42, class_weight='balanced_subsample')\n",
    "\n",
    "brf_results = model_run(brf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make weights\n",
    "neg_count = train_df['TX_FRAUD'].value_counts()[0]\n",
    "pos_count = train_df['TX_FRAUD'].value_counts()[1]\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "# Run XGBoost\n",
    "xgb_model: XGBClassifier = XGBClassifier()\n",
    "xgb_model.objective = 'binary:logistic'\n",
    "xgb_model.scale_pos_weight = scale_pos_weight\n",
    "\n",
    "xgb_results = model_run(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "meta_model = LogisticRegression()\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('brf', brf_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "stacking_results = model_run(stacking_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge all model results into one dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Balanced Random Forest', 'XGBoost', 'Stacking'],\n",
    "    'Precision': [logreg_results['Precision'], brf_results['Precision'], xgb_results['Precision'], stacking_results['Precision']],\n",
    "    'Recall': [logreg_results['Recall'], brf_results['Recall'], xgb_results['Recall'], stacking_results['Recall']],\n",
    "    'F1': [logreg_results['F1'], brf_results['F1'], xgb_results['F1'], stacking_results['F1']],\n",
    "    'PR-AUC': [logreg_results['PR-AUC'], brf_results['PR-AUC'], xgb_results['PR-AUC'], stacking_results['PR-AUC']],\n",
    "})\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "results_df_melt = pd.melt(results_df, id_vars=['Model'], value_vars=['Precision', 'Recall', 'F1', 'PR-AUC'], var_name='Metric', value_name='Score')\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "sns.barplot(y=\"Metric\", x=\"Score\", hue=\"Model\", ax=ax, data=results_df_melt)\n",
    "ax.set(ylabel = \"Model\", xlabel=\"Score\",\n",
    "       title=\"Model Performance Metrics\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these models have their own strengths and weaknesses, and the “best” is based on real-world scenarios.\n",
    "- If we wanted a model that reduces the false positive rate (FPR), which would reduce end customer negative experience, we would focus on precision.\n",
    "- If each fraudulent case was costly to the business, and must be caught while ignoring customer experience, we would prioritize recall.\n",
    "\n",
    "From the graph, the “best” overall model that has balanced performance would be stacking since it has the most balanced results across all four scores. This may be due to it using both BRF and XGBoost outputs to form a new classification model. Even though the BRF has the best recall rate, it also has the worst precision score, leading to a F1 score that is just slightly better than our linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretation and Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we decided to look into after performing the models was seeing if the feature importances were similar between the Balanced Random Forest and XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "brf_importances = pd.DataFrame({'Features': engineered_features, 'BRF': brf_model.feature_importances_})\n",
    "xgb_importances = pd.DataFrame({'Features': engineered_features, 'XGB': xgb_model.feature_importances_})\n",
    "\n",
    "sns.barplot(y=\"Features\", x=\"BRF\", ax=ax[0], data=brf_importances.sort_values(by=\"BRF\", ascending=False))\n",
    "ax[0].set(xlabel = \"Feature Importance Score (BRF)\", ylabel=\"Features\")\n",
    "sns.barplot(y=\"Features\", x=\"XGB\", ax=ax[1], data=xgb_importances.sort_values(by=\"XGB\", ascending=False))\n",
    "ax[1].set(xlabel = \"Feature Importance Score (XGB)\", ylabel=\"Features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models look to have `TERMINAL_FRAUD_RATIO_28D` as the feature with the highest importance score, but the `SPENDING_Z_SCORE_28D` feature appears 3rd in the BRF and 2nd in the XGB feature importance. We decided then to graph the distributions of these features, seeing if these distributions can explain why the models have these features as the highest out of the others. We didn't pay much attention to `TERMINAL_FRAUD_RATIO_7D` as it's 28 day counterpart is more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we looked at the most important: `TERMINAL_FRAUD_RATIO_28D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "terminal_fraud_ratio_28d_nonfraud = pd.DataFrame(df2[df2['TX_FRAUD'] == 0][\"TERMINAL_FRAUD_RATIO_28D\"])\n",
    "terminal_fraud_ratio_28d_fraud = pd.DataFrame(df2[df2['TX_FRAUD'] == 1][\"TERMINAL_FRAUD_RATIO_28D\"])\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "_.tight_layout(h_pad=4)\n",
    "\n",
    "terminal_fraud_ratio_28d_nonfraud.plot(kind='hist', ax=ax, bins=50, logy=True, color='cornflowerblue', alpha=0.5)\n",
    "terminal_fraud_ratio_28d_fraud.plot(kind='hist', ax=ax, bins=50, logy=True, color='limegreen', alpha=0.5)\n",
    "\n",
    "ax.set(xlabel = \"Terminal Fraud Ratio\", ylabel=\"Log Count\",\n",
    "          title=\"Distribution of Terminal Fraud Ratio within a 28-Day Rolling Window\");\n",
    "ax.legend([\"Non-Fraudulent Transactions\", \"Fraudulent Transactions\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph doesn't look too remarkable at first, but for the fraud transactions, the feature does have a spike at around 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we looked at `SPENDING_Z_SCORE_28D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "_.tight_layout(h_pad=4)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "sns.histplot(data=df2[df2['TX_FRAUD'] == 0], x='SPENDING_Z_SCORE_28D', ax=ax[0], bins=100, color='cornflowerblue')\n",
    "ax[0].set(xlabel = \"Customer Spending Z-Scores\", ylabel=\"Count\",\n",
    "          title=\"Distribution of Customer Spending Z-Scores within a 28-Day Rolling Window for Non-Fraudulent Transactions\");\n",
    "sns.histplot(data=df2[df2['TX_FRAUD'] == 1], x='SPENDING_Z_SCORE_28D', ax=ax[1], color='limegreen')\n",
    "ax[1].set(xlabel = \"Customer Spending Z-Scores\", ylabel=\"Count\",\n",
    "          title=\"Distribution of Customer Spending Z-Scores within a 28-Day Rolling Window for Fraudulent Transactions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs show a lot more important information. For non-fraudulent transactions, the distributions is centered at 0 and tapers off, almost looking like a normal distribution of some sort. As for the fraudulent transactions, the distribution also has a majority of the data around 0, but the range of values is much higher, and these higher Z-scores have more transactions within them, possibly meaning that for transactions with a high customer spending Z-score, they may have a good chance of being fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to fit a model onto a dataset, it is important to understand the balance of false positives and false negatives the model makes when understanding its performance, and in the case of fraud detection, it’s very important. If the model makes too many false positives, more legit transactions will be flagged as fraudulent, possibly leading to customer complaints and possible customer losses in banking companies, but if the model makes too many false negatives, more fraud transactions will be treated as legit, leading to potentially millions of dollars being stolen. In fraud classification, precision and recall often have an inverse relationship, improving one typically makes the other worse and it depends on what we focus on based on specific use cases. We choose to use more complex models such as random forests and XGBoost to fit our data instead of simpler models such as logistic regression since it came down to being able to classify fraudulent transactions to our best ability over interpreting the results more easily.\n",
    "\n",
    "Simpler models like logistic regression output coefficients that can explain each feature's impact on the response assuming the other features are held constant, but these kinds of models assume a linear or parametric relationship between the features and the response, thus not being able to fit complex relationships well.\n",
    "\n",
    "Complex models like BRF and XGBoost are able to fit complex relationships well without the assumptions, leading to them being considered \"accurate\" models. But with that accuracy comes a downside: lack of interpretability. Most of these complex models are what's known as \"black-box models\", which means simply that we don't know what the underlying nature is the model sees. We can use things like feature importance for BRF and XGBoost to try and understand what features the model believes are important, but if we want to know a direct relationship between the features and the response, these models won't help. Without those direct relationships, there is no clear justification on why each transaction was classified as fraudulent without easily human understandable explanations, which can create regulatory or legal challenges when implementing these into real world scenarios and can cause trust in the systems that are being put in place.\n",
    "\n",
    "With those three options, we had the ability to choose a model that could be more accurate or more explainable, and based on use cases can be used for this objective, we chose the stacked model because it was overall the best at determining fraudulent cases while also minimizing false positive rates, but the other models could be optimized to meet our goals as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the models and their evaluations complete, we can now look at real-world implications. While we choose the metrics that were balanced, the choice of the model should align with business risks and operational needs within fraud mitigation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting our models onto the feature-engineered dataset, we specifically chose the features from our exploratory data analysis that would have an impact on detecting fraud, but there could possibly be other features we decided not to use that could have helped to improve performance. With additional time, we would have looked into other features, such as CUSTOMER_ID or TX_DATETIME, and see if they changed the performance of our model. Since the time and computational power was a consideration, we didn’t focus on tuning any hyperparameters for our models. With each of these models being considered a base model, they performed better than expected. If we had additional time and computational power, we could have used additional time to improve these models by adjusting hyperparameters to achieve a higher F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, we decided to run our original dataset into a balanced random forest model to see how well it performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "split_point = df['TX_TIME_DAYS'].quantile(0.8)\n",
    "base_train_df = df[df['TX_TIME_DAYS'] < split_point].copy()\n",
    "base_test_df = df[df['TX_TIME_DAYS'] >= split_point].copy()\n",
    "\n",
    "# Extract features and target variable\n",
    "base_features = ['CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT', 'TX_TIME_SECONDS', 'TX_TIME_DAYS', 'TX_HOUR', 'TX_DAYOFWEEK']\n",
    "target = 'TX_FRAUD'\n",
    "\n",
    "X_train, y_train = base_train_df[base_features], base_train_df[target]\n",
    "X_test, y_test = base_test_df[base_features], base_test_df[target]\n",
    "\n",
    "# Initialize Balanced Random Forest\n",
    "base_brf_model = BalancedRandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "base_brf_results = model_run(base_brf_model, train_df = base_train_df, test_df = base_test_df, features = base_features)\n",
    "\n",
    "pd.DataFrame({\"Score\": base_brf_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a precision rate of 0.02, recall rate of 0.43, and F1 score of 0.04, the original dataset does not hold up well to the feature-engineered dataset. This makes sense as we assume the underlying nature of the data includes information that is not explained in the original dataset, thus requiring the feature engineering to bring out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our chosen model, what decided to be the “best” we would, in this six month time period the dataset spanned, have flagged roughly 5,820 incidents of fraud that are not and wouldn’t of caught roughly 4,390 fraudulent activity. Each of these causes monetary cost to the organization that would be deployed, and until additional tuning of these model implemented, would not recommend using them in real world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Figure out how to wrap text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.security.org/digital-safety/credit-card-fraud-report/\n",
    "- https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "- https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_3_GettingStarted/SimulatedDataset.html\n",
    "- https://www.aporia.com/learn/ultimate-guide-to-precision-recall-auc-understanding-calculating-using-pr-auc-in-ml/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
