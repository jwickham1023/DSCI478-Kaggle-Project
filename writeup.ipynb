{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# DSCI478 Kaggle Project - Credit Card Fraud Detection\n",
    "### Nick Brady, Jakob Wickham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for us:\n",
    "- If you want the PDF to not display a cell, click the three dots on the cell, click \"Add Cell Tag\", and put \"remove_cell\"\n",
    "- If you want to hide the code instead, put \"remove_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TRANSACTION_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TX_DATETIME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CUSTOMER_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TERMINAL_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TX_AMOUNT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TX_TIME_SECONDS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TX_TIME_DAYS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TX_FRAUD",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TX_FRAUD_SCENARIO",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e6835c27-304c-44f1-a351-9792b95c262b",
       "rows": [
        [
         "0",
         "0",
         "2018-04-01 00:00:31",
         "596",
         "3156",
         "57.16",
         "31",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "1",
         "2018-04-01 00:02:10",
         "4961",
         "3412",
         "81.51",
         "130",
         "0",
         "0",
         "0"
        ],
        [
         "2",
         "2",
         "2018-04-01 00:07:56",
         "2",
         "1365",
         "146.0",
         "476",
         "0",
         "0",
         "0"
        ],
        [
         "3",
         "3",
         "2018-04-01 00:09:29",
         "4128",
         "8737",
         "64.49",
         "569",
         "0",
         "0",
         "0"
        ],
        [
         "4",
         "4",
         "2018-04-01 00:10:34",
         "927",
         "9906",
         "50.99",
         "634",
         "0",
         "0",
         "0"
        ],
        [
         "5",
         "5",
         "2018-04-01 00:10:45",
         "568",
         "8803",
         "44.71",
         "645",
         "0",
         "0",
         "0"
        ],
        [
         "6",
         "6",
         "2018-04-01 00:11:30",
         "2803",
         "5490",
         "96.03",
         "690",
         "0",
         "0",
         "0"
        ],
        [
         "7",
         "7",
         "2018-04-01 00:11:44",
         "4684",
         "2486",
         "24.36",
         "704",
         "0",
         "0",
         "0"
        ],
        [
         "8",
         "8",
         "2018-04-01 00:11:53",
         "4128",
         "8354",
         "26.34",
         "713",
         "0",
         "0",
         "0"
        ],
        [
         "9",
         "9",
         "2018-04-01 00:13:44",
         "541",
         "6212",
         "59.07",
         "824",
         "0",
         "0",
         "0"
        ],
        [
         "10",
         "10",
         "2018-04-01 00:16:59",
         "4554",
         "2198",
         "58.06",
         "1019",
         "0",
         "0",
         "0"
        ],
        [
         "11",
         "11",
         "2018-04-01 00:17:44",
         "2000",
         "7997",
         "66.38",
         "1064",
         "0",
         "0",
         "0"
        ],
        [
         "12",
         "12",
         "2018-04-01 00:18:01",
         "1948",
         "3372",
         "54.51",
         "1081",
         "0",
         "0",
         "0"
        ],
        [
         "13",
         "13",
         "2018-04-01 00:19:22",
         "2938",
         "1516",
         "22.0",
         "1162",
         "0",
         "0",
         "0"
        ],
        [
         "14",
         "14",
         "2018-04-01 00:19:49",
         "2989",
         "4111",
         "28.42",
         "1189",
         "0",
         "0",
         "0"
        ],
        [
         "15",
         "15",
         "2018-04-01 00:20:03",
         "3842",
         "1693",
         "26.23",
         "1203",
         "0",
         "0",
         "0"
        ],
        [
         "16",
         "16",
         "2018-04-01 00:20:25",
         "4361",
         "4322",
         "92.69",
         "1225",
         "0",
         "0",
         "0"
        ],
        [
         "17",
         "17",
         "2018-04-01 00:20:40",
         "4177",
         "6270",
         "65.15",
         "1240",
         "0",
         "0",
         "0"
        ],
        [
         "18",
         "18",
         "2018-04-01 00:20:48",
         "3700",
         "471",
         "87.38",
         "1248",
         "0",
         "0",
         "0"
        ],
        [
         "19",
         "19",
         "2018-04-01 00:21:06",
         "3671",
         "4223",
         "68.98",
         "1266",
         "0",
         "0",
         "0"
        ],
        [
         "20",
         "20",
         "2018-04-01 00:21:09",
         "1270",
         "931",
         "140.75",
         "1269",
         "0",
         "0",
         "0"
        ],
        [
         "21",
         "21",
         "2018-04-01 00:22:14",
         "2899",
         "6019",
         "36.58",
         "1334",
         "0",
         "0",
         "0"
        ],
        [
         "22",
         "22",
         "2018-04-01 00:23:32",
         "4582",
         "7998",
         "48.3",
         "1412",
         "0",
         "0",
         "0"
        ],
        [
         "23",
         "23",
         "2018-04-01 00:25:24",
         "508",
         "9687",
         "139.45",
         "1524",
         "0",
         "0",
         "0"
        ],
        [
         "24",
         "24",
         "2018-04-01 00:26:37",
         "2323",
         "1257",
         "63.19",
         "1597",
         "0",
         "0",
         "0"
        ],
        [
         "25",
         "25",
         "2018-04-01 00:26:54",
         "1152",
         "8621",
         "54.08",
         "1614",
         "0",
         "0",
         "0"
        ],
        [
         "26",
         "26",
         "2018-04-01 00:27:02",
         "2389",
         "9739",
         "36.91",
         "1622",
         "0",
         "0",
         "0"
        ],
        [
         "27",
         "27",
         "2018-04-01 00:27:27",
         "967",
         "9401",
         "74.32",
         "1647",
         "0",
         "0",
         "0"
        ],
        [
         "28",
         "28",
         "2018-04-01 00:28:07",
         "616",
         "7758",
         "12.18",
         "1687",
         "0",
         "0",
         "0"
        ],
        [
         "29",
         "29",
         "2018-04-01 00:28:12",
         "4205",
         "2288",
         "69.66",
         "1692",
         "0",
         "0",
         "0"
        ],
        [
         "30",
         "30",
         "2018-04-01 00:29:05",
         "1106",
         "1317",
         "8.75",
         "1745",
         "0",
         "0",
         "0"
        ],
        [
         "31",
         "31",
         "2018-04-01 00:29:47",
         "2605",
         "6231",
         "38.31",
         "1787",
         "0",
         "0",
         "0"
        ],
        [
         "32",
         "32",
         "2018-04-01 00:30:05",
         "360",
         "1611",
         "92.74",
         "1805",
         "0",
         "0",
         "0"
        ],
        [
         "33",
         "33",
         "2018-04-01 00:30:45",
         "4052",
         "3260",
         "5.69",
         "1845",
         "0",
         "0",
         "0"
        ],
        [
         "34",
         "34",
         "2018-04-01 00:31:27",
         "3944",
         "2696",
         "75.0",
         "1887",
         "0",
         "0",
         "0"
        ],
        [
         "35",
         "35",
         "2018-04-01 00:31:51",
         "1753",
         "8676",
         "115.78",
         "1911",
         "0",
         "0",
         "0"
        ],
        [
         "36",
         "36",
         "2018-04-01 00:32:35",
         "183",
         "4972",
         "39.3",
         "1955",
         "0",
         "0",
         "0"
        ],
        [
         "37",
         "37",
         "2018-04-01 00:32:53",
         "4563",
         "4309",
         "31.18",
         "1973",
         "0",
         "0",
         "0"
        ],
        [
         "38",
         "38",
         "2018-04-01 00:33:22",
         "1546",
         "2897",
         "37.39",
         "2002",
         "0",
         "0",
         "0"
        ],
        [
         "39",
         "39",
         "2018-04-01 00:33:56",
         "4691",
         "3227",
         "44.81",
         "2036",
         "0",
         "0",
         "0"
        ],
        [
         "40",
         "40",
         "2018-04-01 00:33:57",
         "4613",
         "4971",
         "33.95",
         "2037",
         "0",
         "0",
         "0"
        ],
        [
         "41",
         "41",
         "2018-04-01 00:34:14",
         "3445",
         "7820",
         "36.7",
         "2054",
         "0",
         "0",
         "0"
        ],
        [
         "42",
         "42",
         "2018-04-01 00:34:29",
         "731",
         "9969",
         "53.55",
         "2069",
         "0",
         "0",
         "0"
        ],
        [
         "43",
         "43",
         "2018-04-01 00:34:45",
         "3425",
         "2930",
         "119.56",
         "2085",
         "0",
         "0",
         "0"
        ],
        [
         "44",
         "44",
         "2018-04-01 00:36:18",
         "2916",
         "1315",
         "16.48",
         "2178",
         "0",
         "0",
         "0"
        ],
        [
         "45",
         "46",
         "2018-04-01 00:36:39",
         "2033",
         "2061",
         "48.03",
         "2199",
         "0",
         "0",
         "0"
        ],
        [
         "46",
         "45",
         "2018-04-01 00:36:39",
         "855",
         "4297",
         "78.32",
         "2199",
         "0",
         "0",
         "0"
        ],
        [
         "47",
         "47",
         "2018-04-01 00:37:06",
         "4999",
         "3885",
         "38.41",
         "2226",
         "0",
         "0",
         "0"
        ],
        [
         "48",
         "48",
         "2018-04-01 00:37:45",
         "1660",
         "1562",
         "43.52",
         "2265",
         "0",
         "0",
         "0"
        ],
        [
         "49",
         "49",
         "2018-04-01 00:38:22",
         "4853",
         "1417",
         "10.8",
         "2302",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 1754155
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_TIME_SECONDS</th>\n",
       "      <th>TX_TIME_DAYS</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_FRAUD_SCENARIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-01 00:00:31</td>\n",
       "      <td>596</td>\n",
       "      <td>3156</td>\n",
       "      <td>57.16</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-04-01 00:02:10</td>\n",
       "      <td>4961</td>\n",
       "      <td>3412</td>\n",
       "      <td>81.51</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-04-01 00:07:56</td>\n",
       "      <td>2</td>\n",
       "      <td>1365</td>\n",
       "      <td>146.00</td>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-04-01 00:09:29</td>\n",
       "      <td>4128</td>\n",
       "      <td>8737</td>\n",
       "      <td>64.49</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-01 00:10:34</td>\n",
       "      <td>927</td>\n",
       "      <td>9906</td>\n",
       "      <td>50.99</td>\n",
       "      <td>634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754150</th>\n",
       "      <td>1754150</td>\n",
       "      <td>2018-09-30 23:56:36</td>\n",
       "      <td>161</td>\n",
       "      <td>655</td>\n",
       "      <td>54.24</td>\n",
       "      <td>15810996</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754151</th>\n",
       "      <td>1754151</td>\n",
       "      <td>2018-09-30 23:57:38</td>\n",
       "      <td>4342</td>\n",
       "      <td>6181</td>\n",
       "      <td>1.23</td>\n",
       "      <td>15811058</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754152</th>\n",
       "      <td>1754152</td>\n",
       "      <td>2018-09-30 23:58:21</td>\n",
       "      <td>618</td>\n",
       "      <td>1502</td>\n",
       "      <td>6.62</td>\n",
       "      <td>15811101</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754153</th>\n",
       "      <td>1754153</td>\n",
       "      <td>2018-09-30 23:59:52</td>\n",
       "      <td>4056</td>\n",
       "      <td>3067</td>\n",
       "      <td>55.40</td>\n",
       "      <td>15811192</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754154</th>\n",
       "      <td>1754154</td>\n",
       "      <td>2018-09-30 23:59:57</td>\n",
       "      <td>3542</td>\n",
       "      <td>9849</td>\n",
       "      <td>23.59</td>\n",
       "      <td>15811197</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754155 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRANSACTION_ID          TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  \\\n",
       "0                     0  2018-04-01 00:00:31          596         3156   \n",
       "1                     1  2018-04-01 00:02:10         4961         3412   \n",
       "2                     2  2018-04-01 00:07:56            2         1365   \n",
       "3                     3  2018-04-01 00:09:29         4128         8737   \n",
       "4                     4  2018-04-01 00:10:34          927         9906   \n",
       "...                 ...                  ...          ...          ...   \n",
       "1754150         1754150  2018-09-30 23:56:36          161          655   \n",
       "1754151         1754151  2018-09-30 23:57:38         4342         6181   \n",
       "1754152         1754152  2018-09-30 23:58:21          618         1502   \n",
       "1754153         1754153  2018-09-30 23:59:52         4056         3067   \n",
       "1754154         1754154  2018-09-30 23:59:57         3542         9849   \n",
       "\n",
       "         TX_AMOUNT  TX_TIME_SECONDS  TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \n",
       "0            57.16               31             0         0                  0  \n",
       "1            81.51              130             0         0                  0  \n",
       "2           146.00              476             0         0                  0  \n",
       "3            64.49              569             0         0                  0  \n",
       "4            50.99              634             0         0                  0  \n",
       "...            ...              ...           ...       ...                ...  \n",
       "1754150      54.24         15810996           182         0                  0  \n",
       "1754151       1.23         15811058           182         0                  0  \n",
       "1754152       6.62         15811101           182         0                  0  \n",
       "1754153      55.40         15811192           182         0                  0  \n",
       "1754154      23.59         15811197           182         0                  0  \n",
       "\n",
       "[1754155 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2024 alone, 134 million Americans have been victims of credit card fraud, with unauthorized purchases accounting for 6.2 billion dollars annually. This trend of fraud is on the rise in financial and business challenges. (Reference 1).  While consumers are generally protected from fraudulent transactions, the impact is beyond the negative experience; financial institutions and businesses bear the cost.\n",
    "\n",
    "To reduce these risks, financial institutions leverage Machine Learning (ML) and other statistical models to detect and prevent transactions from occurring. Creating effective fraud detection models has challenges, including correctly classifying fraudulent transactions, minimizing customer impact with false positives, and dealing with multiple fraud vectors.\n",
    "\n",
    "In this project, we will explore the process of creating a fraud detection model, the data set we used, feature engineering, model evaluation metrics, and the challenges of imbalanced data sets. Since fraudulent activity is rare, accuracy metrics are not applicable when determining model evaluation as they only give a score on how well it detects *non-fraud*. In this project we will be using precision, recall, F1 score and the Precision-Recall Curve (PR - AUC) to provide an accurate representation of the model given the challenges in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of the information for credit card purchases, we used a synthetic data set (References 2 & 3), which allows us to explore fraud detection techniques while also maintaining privacy and security concerns. At the end of this section is a list of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This synthetic data can help us in research and model experimentation. It introduces lamination: the data may not reflect real world approaches that fraudsters use, customer transactions may lack variability, and `TX_FRAUD_SCENARIO` can create potential data leakage.\n",
    "With those limitations in mind, we want to be able to focus on building feature engineering and methods that can generalize well to actual scenarios. We must first explore the data cleaning and processing steps taken to prepare the data set for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "    \\begin{tabular}{| l | p{10cm} |}\n",
    "        \\hline\n",
    "        \\textbf{Feature} & \\textbf{Description} \\\\\n",
    "        \\hline\n",
    "        TRANSACTION\\_ID & A unique identifier for the transaction \\\\\n",
    "        \\hline\n",
    "        TX\\_DATETIME & Date and time at which the transaction occurs \\\\\n",
    "        \\hline\n",
    "        CUSTOMER\\_ID & The identifier for the customer. Each customer has a unique identifier \\\\\n",
    "        \\hline\n",
    "        TERMINAL\\_ID & The identifier for the merchant (or, more precisely, the terminal). Each terminal has a unique identifier \\\\\n",
    "        \\hline\n",
    "        TX\\_AMOUNT & The amount of the transaction \\\\\n",
    "        \\hline\n",
    "        TX\\_FRAUD & A binary variable with the value for a legitimate transaction or the value for a fraudulent transaction \\\\\n",
    "        \\hline\n",
    "        TX\\_TIME\\_SECONDS & Timestamp of transaction in seconds \\\\\n",
    "        \\hline\n",
    "        TX\\_TIME\\_DAYS & Timestamp of transaction in days \\\\\n",
    "        \\hline\n",
    "        TX\\_FRAUD\\_SCENARIO & Numerical indicator of the type of fraud scenario \\\\\n",
    "        \\hline\n",
    "    \\end{tabular}\n",
    "    \\caption{Transaction Features and Their Descriptions}\n",
    "    \\label{tab:transaction_features}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting exploratory data analysis and model development, we had to clean data and process the data.\n",
    "\n",
    "Since this data was synthetically generated, no missing (NaN) values were present. In the data, the `TX_FRAUD_SCENARIO` column was dropped to prevent data leakage as it contained information directly to the fraud classification. `TX_DATETIME` was transformed into multiple features--`TX_HOUR` (hour of the transaction) and `TX_DAYOFWEEK` (day of the transaction week)--to capture temporal patterns. Once that transformation was completed, `TX_DATETIME` was dropped due to redundant data.\n",
    "\n",
    "Completing these preprocessing steps ensure that the dataset was structured for feature extraction while also minimizing data leakage. Transforming `TX_DATETIME` allowed us to capture temporal fraud patterns. With these preprocessing steps done, the data set is now ready for exploratory data analysis (EDA), where we will be determining patterns in fraudulent and non-fraudulent transactions, key trends and assess potential predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "\n",
    "df.columns = df.columns.str.strip().str.upper()\n",
    "\n",
    "# Drop unnecessary column\n",
    "df.drop(columns=['TX_FRAUD_SCENARIO'], inplace=True)\n",
    "\n",
    "# Convert TX_DATETIME and extract key time features\n",
    "df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])\n",
    "df['TX_HOUR'] = df['TX_DATETIME'].dt.hour\n",
    "df['TX_DAYOFWEEK'] = df['TX_DATETIME'].dt.dayofweek\n",
    "df.drop(columns=['TX_DATETIME'], inplace=True)\n",
    "\n",
    "\n",
    "# Time-Based Split\n",
    "split_point = df['TX_TIME_DAYS'].quantile(0.8)\n",
    "train_df = df[df['TX_TIME_DAYS'] < split_point].copy()\n",
    "test_df = df[df['TX_TIME_DAYS'] >= split_point].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we performed basic data exploration to learn more about our data set. One of the first things in our exploration analysis that was noticeable was that the data set was highly imbalanced, with only fraudulent activity making up a small proportion of the total (which was a given considering we're working with fraud). Being aware of class balance is important as it affects model performance and evaluation metrics. Additionally, we analyzed the average transaction that customers typically make with their cards. With this information we wanted to determine if going above or below the average transaction was more likely fraudulent. Since this was a significant factor in fraud detection, we further explored whether a customer’s personal average carried more of a determining factor than global average (See Figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our findings showed that incorporating a customer’s historical spending behavior improved the chance of finding fraudulent activity, which could show that people who commit fraud are trying to stay within the expected spending patterns of the individual (See Figure 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also investigated whether certain terminals were consistent with fraudulent activity (See Figure 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This revealed that a small number of terminals were responsible for a large portion of fraud, often occurring over consecutive days. However, there was no fixed pattern in the number of days fraud occurred and no clear distinction between whether a certain day should be a concern (See Figure 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, we had to implement a way in our classification approach that acknowledges that past fraudulent activity at a terminal is a strong factor, but not all future transactions at that terminal should be flagged as fraud.\n",
    "\n",
    "Our exploration analysis showed many key significant insights into fraud detection, which include class imbalance, transaction amount and terminal level indicators. Those findings gave us information that was necessary to implement our feature engineering and modeling approach to account for the customer and terminal behavior patterns and minimize our bias in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a robust fraud detection, we engineered features that capture both population fraud trends and individual customer-based spending behaviors. This creates a model that can detect a wide range of fraud tactics while also identifying personalized anomalies that could indicate fraudulent activity at a customer level.\n",
    "\n",
    "Transaction amount serves as a critical indication in fraudulent activity. We applied a Z-score normalization to standardize transactions relative to the dataset mean, which ensures that large transaction values were flagged as unusual. However, some of the fraudulent activities attempted to fall within the customer expected ranges but still would appear abnormal compared to overall transaction patterns. Since specific terminals were exploited, we implemented rolling seven-and twenty-eight day rolling fraud rates. Acknowledging that a terminal who has had fraudulent activity doesn’t mean they will always be fraudulent we implemented an exponential decay factor to ensure old occurrences of fraud had a diminishing influence in our model and attempting to prevent model but still capturing consistent fraud risk. To further improve our population fraud trend, we implemented a Local Outlier Factor (LOF), and an unsupervised anomaly detection method that assigned a likelihood of fraud score based on its behavior. This method creates context-based detecting and maintains to not solely rely on extreme transaction values. During the feature engineering process we considered using an Isolation Forest for our unsupervised choice, but the anomaly score made our model rely on the Isolation Forest prediction, which reduced its ability to learn other fraudulent trends.\n",
    "\n",
    "Listed below shows all population-based features that were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# KaTeX parsing error, but it will parse correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "    \\begin{tabular}{| l | p{10cm} |}\n",
    "        \\hline\n",
    "        \\textbf{Feature} & \\textbf{Purpose} \\\\\n",
    "        \\hline\n",
    "        TX\\_AMOUNT\\_Z\\_Score & Check for extreme transaction amounts compared to data set's mean \\\\\n",
    "        \\hline\n",
    "        TX\\_AMOUNT\\_Percentile & Check transaction amounts relative to all others \\\\\n",
    "        \\hline\n",
    "        TERMINAL\\_FRAUD\\_RATIO & Terminal-level fraud check that decays over time \\\\\n",
    "        \\hline\n",
    "        ANOMALY\\_SCORE (LOF) & Unsupervised learning to apply a score beyond deviation of previous features \\\\\n",
    "        \\hline\n",
    "    \\end{tabular}\n",
    "    \\caption{Feature Description and Purpose}\n",
    "    \\label{tab:feature_purpose}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population-based trend provides a big picture view of our sample, but fraudulent transactions often are different from customer behavior rather than population norm. To recognize this, we engineered features that will adapt to each customer's unique spending habits instead of over relying on dataset wide fraud indicators. The key feature of detecting fraud at a customer level is track spending behavior over time. We computed a 14-day rolling window that tracks whether a customer suddenly makes a significant change in their spending habits. This can help with identifying fraud scenarios where fraudsters make large transactions over a short period, deviation from previous spending habits. We normalized the transaction amount based on each customer's historical data instead of the entire data set. This implementation prevents the model from flagging frequently high spending larger transactions, while still detecting unexpected large purchases from lower spending customers.\n",
    "\n",
    "The table below (Figure 6) shows all customer-based features that were created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "    \\begin{tabular}{| l | p{10cm} |}\n",
    "        \\hline\n",
    "        \\textbf{Customer-Based Features} & \\textbf{Purpose} \\\\\n",
    "        \\hline\n",
    "        SPENDING\\_RATIO\\_CHANGE & Checks for sudden shifts in customer spending behavior \\\\\n",
    "        \\hline\n",
    "        SPENDING\\_Z\\_SCORE\\_28D & Identifies how unusual a transaction is for a specific customer based on their historical spending patterns \\\\\n",
    "        \\hline\n",
    "    \\end{tabular}\n",
    "    \\caption{Customer-Based Features and Their Purpose}\n",
    "    \\label{tab:customer_features}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at both the population and customer-based fraud detection the purpose is to create a model that can identify fraud trends that are occurring at large scale but also adapting to customer level behavior. This combination with dynamic adjusting and personalized profiles attempts to create a fraud detection model that is both effective and adaptable at identifying ever changing fraud tactics.\n",
    "With the creation of these engineered features, we now will focus on model selection and evaluating different approaches at models to classify fraudulent activity effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Feature Engineering Rolling Windows with decay\n",
    "scaler = StandardScaler()\n",
    "train_df['TX_AMOUNT_Z_SCORE'] = scaler.fit_transform(train_df[['TX_AMOUNT']])\n",
    "test_df['TX_AMOUNT_Z_SCORE'] = scaler.transform(test_df[['TX_AMOUNT']])\n",
    "\n",
    "train_df['TX_AMOUNT_PERCENTILE'] = train_df['TX_AMOUNT'].rank(pct=True)\n",
    "test_df['TX_AMOUNT_PERCENTILE'] = test_df['TX_AMOUNT'].rank(pct=True)\n",
    "\n",
    "# Optimize Decay Factor \n",
    "decay_values = np.linspace(0.8, 0.99, 20)  \n",
    "best_decay, best_corr = 0.95, float('-inf')  \n",
    "\n",
    "for decay in decay_values:\n",
    "    temp_df = train_df.copy()\n",
    "    temp_df['TERMINAL_FRAUD_RATIO_28D'] = (\n",
    "        temp_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "        .shift(1)\n",
    "        .ewm(alpha=1-decay)\n",
    "        .mean()\n",
    "    )\n",
    "    correlation = temp_df['TERMINAL_FRAUD_RATIO_28D'].corr(temp_df['TX_FRAUD'])\n",
    "    \n",
    "    if correlation > best_corr:\n",
    "        best_corr = correlation\n",
    "        best_decay = decay\n",
    "\n",
    "optimal_decay = best_decay  # Best decay factor based on correlation\n",
    "\n",
    "# Apply the optimized decay factor\n",
    "train_df['TERMINAL_FRAUD_RATIO_28D'] = (\n",
    "    train_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .ewm(alpha=1-optimal_decay)\n",
    "    .mean()\n",
    ")\n",
    "test_df['TERMINAL_FRAUD_RATIO_28D'] = (\n",
    "    test_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .ewm(alpha=1-optimal_decay)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Terminal Fraud Ratios\n",
    "train_df['TERMINAL_FRAUD_RATIO_7D'] = (\n",
    "    train_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .rolling(7, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "test_df['TERMINAL_FRAUD_RATIO_7D'] = (\n",
    "    test_df.groupby('TERMINAL_ID')['TX_FRAUD']\n",
    "    .shift(1)\n",
    "    .rolling(7, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Customer Spending Trends\n",
    "train_df['SPENDING_RATIO_CHANGE'] = (\n",
    "    train_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: x.pct_change().rolling(14, min_periods=1).mean())\n",
    ")\n",
    "test_df['SPENDING_RATIO_CHANGE'] = (\n",
    "    test_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: x.pct_change().rolling(14, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "train_df['SPENDING_Z_SCORE_28D'] = (\n",
    "    train_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: (x - x.mean()) / (x.std() + 1))\n",
    ")\n",
    "test_df['SPENDING_Z_SCORE_28D'] = (\n",
    "    test_df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "    .transform(lambda x: (x - x.mean()) / (x.std() + 1))\n",
    ")\n",
    "\n",
    "\n",
    "# Anomaly Detection (Local Outlier Factor)\n",
    "features_for_lof = ['TX_AMOUNT_Z_SCORE', 'SPENDING_RATIO_CHANGE', 'TERMINAL_FRAUD_RATIO_28D']\n",
    "train_df[features_for_lof] = train_df[features_for_lof].fillna(train_df[features_for_lof].median())\n",
    "test_df[features_for_lof] = test_df[features_for_lof].fillna(test_df[features_for_lof].median())\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.005, novelty=True)\n",
    "lof.fit(train_df[features_for_lof].values)\n",
    "\n",
    "train_df['ANOMALY_SCORE'] = lof.decision_function(train_df[features_for_lof].values)\n",
    "test_df['ANOMALY_SCORE'] = lof.decision_function(test_df[features_for_lof].values)\n",
    "\n",
    "# Normalize Anomaly Scores\n",
    "scaler = MinMaxScaler()\n",
    "train_df['ANOMALY_SCORE'] = scaler.fit_transform(train_df[['ANOMALY_SCORE']])\n",
    "test_df['ANOMALY_SCORE'] = scaler.transform(test_df[['ANOMALY_SCORE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Isolation Forest, Balanced Random Forest, etc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretation and Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Graphs such as feature importance and within-feature / within-fraud boxplots*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Talk about the three scenarios that we used to make a transaction fraud*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Figure out how to wrap text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.security.org/digital-safety/credit-card-fraud-report/\n",
    "- https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "- https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_3_GettingStarted/SimulatedDataset.html\n",
    "- https://www.aporia.com/learn/ultimate-guide-to-precision-recall-auc-understanding-calculating-using-pr-auc-in-ml/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
